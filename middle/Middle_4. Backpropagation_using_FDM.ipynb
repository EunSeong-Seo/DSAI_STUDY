{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":533,"status":"ok","timestamp":1654746510244,"user":{"displayName":"‍서은성[ 대학원석사과정재학 / 자동차융합학과 ]","userId":"07531877999761044008"},"user_tz":-540},"id":"LT8DVFGlWNQa"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import csv as csv\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"ly4uoe7bWHEX","executionInfo":{"status":"ok","timestamp":1654746511242,"user_tz":-540,"elapsed":2,"user":{"displayName":"‍서은성[ 대학원석사과정재학 / 자동차융합학과 ]","userId":"07531877999761044008"}}},"outputs":[],"source":["#유한차분법을 이용한 numerical_gradient 함수 \n","\n","def numerical_gradient(f, x):\n","    h = 1e-4 # 0.0001\n","    grad = np.zeros_like(x)\n","    \n","    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n","    while not it.finished:\n","        idx = it.multi_index\n","        tmp_val = x[idx]\n","        x[idx] = float(tmp_val) + h\n","        fxh1 = f(x) # f(x+h)\n","        \n","        x[idx] = tmp_val - h \n","        fxh2 = f(x) # f(x-h)\n","        grad[idx] = (fxh1 - fxh2) / (2*h)\n","        \n","        x[idx] = tmp_val # 값 복원\n","        it.iternext()   \n","        \n","    return grad"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"g_CrDLcFJne5","executionInfo":{"status":"ok","timestamp":1654746511568,"user_tz":-540,"elapsed":4,"user":{"displayName":"‍서은성[ 대학원석사과정재학 / 자동차융합학과 ]","userId":"07531877999761044008"}}},"outputs":[],"source":["#코드 구현에 필요한 수식\n","\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))    \n","\n","def softmax(x):\n","    if x.ndim == 2:\n","        x = x.T\n","        x = x - np.max(x, axis=0)\n","        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n","        return y.T \n","\n","def sigmoid_grad(x):\n","    return (1.0 - sigmoid(x)) * sigmoid(x)\n","\n","    \n","def cross_entropy_error(y, t):\n","    if y.ndim == 1:\n","        t = t.reshape(1, t.size)\n","        y = y.reshape(1, y.size)\n","        \n","    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n","    if t.size == y.size:\n","        t = t.argmax(axis=1)\n","             \n","    batch_size = y.shape[0]\n","    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"04uLS24NVpH4","executionInfo":{"status":"ok","timestamp":1654746512273,"user_tz":-540,"elapsed":2,"user":{"displayName":"‍서은성[ 대학원석사과정재학 / 자동차융합학과 ]","userId":"07531877999761044008"}}},"outputs":[],"source":["# coding: utf-8\n","\n","class TwoLayerNet:\n","    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n","        self.params = {}\n","        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n","        self.params['b1'] = np.zeros(hidden_size)\n","        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n","        self.params['b2'] = np.zeros(output_size)\n","\n","    # 예측\n","    def predict(self, x):\n","        W1, W2 = self.params['W1'], self.params['W2']\n","        b1, b2 = self.params['b1'], self.params['b2']\n","        # Affine -> Sigmoid -> Affine -> Softmax\n","        a1 = np.dot(x, W1) + b1\n","        z1 = sigmoid(a1)\n","        a2 = np.dot(z1, W2) + b2\n","        y = softmax(a2)\n","        \n","        return y\n","    \n","    # 손실함수    \n","    # x : 입력 데이터, t : 정답 레이블\n","    def loss(self, x, t):\n","        y = self.predict(x)\n","        return cross_entropy_error(y, t)\n","    \n","    # 정확도 측정\n","    def accuracy(self, x, t):\n","        y = self.predict(x)\n","        # axis = 1일때 즉, 행에서 가장 값이 큰 인덱스를 리턴한다.\n","        y = np.argmax(y, axis=1)\n","        t = np.argmax(t, axis=1)\n","        \n","        accuracy = np.sum(y == t) / float(x.shape[0])\n","        return accuracy\n","        \n","    def numerical_gradient(self, x, t):\n","        loss_W = lambda W: self.loss(x, t)\n","        \n","        grads = {}\n","        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n","        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n","        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n","        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n","        \n","        return grads"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-AZEeVZ_WRAE","colab":{"base_uri":"https://localhost:8080/"},"outputId":"561e547a-c185-47e5-a73c-93e7a39dd9d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["train acc, test acc | 0.0993, 0.1032\n"]}],"source":["\n","\n","# 데이터 읽기\n","# normalize를 True로 하는 의미 : 픽셀 값을 255로 나눠 0~1 사이 값으로 나타내겠다는 뜻\n","# one_hot_label을 True로 하는 의미 : 0-9 정수를 원핫벡터로 변환\n","# flatten은 디폴트가 True 이므로 여기선 생략\n","\n","from mnist import load_mnist\n","\n","(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n","\n","network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n","\n","# 하이퍼파라미터\n","iters_num = 10000  # 반복 횟수를 10000번으로 설정\n","train_size = x_train.shape[0] # x_train.shape() = (60000, 784) 이므로 train_size는 60000 이다.\n","batch_size = 100   # 미니배치 크기\n","learning_rate = 0.1\n","\n","train_loss_list = []\n","train_acc_list = []\n","test_acc_list = []\n","\n","# 1 에폭 당 반복 수 (epoch이란, 훈련 데이터를 다 소진하는 하나의 구간을 의미함)\n","# ex. MNIST에서는 1에폭당 6만장의 훈련 데이터가 사용된다.\n","# 아래 코드의 의미는 한 에폭당 배치 사이즈만큼 즉 (train_size / batch_size = 600번) 반복된다는 뜻이다.\n","iter_per_epoch = max(train_size / batch_size, 1)\n","\n","for i in range(iters_num):\n","    # 미니배치 획득 \n","    # train_size 이하의 정수 중 batch_size 만큼을 랜덤 추출하라는 뜻\n","    batch_mask = np.random.choice(train_size, batch_size)\n","    # 인덱스가 랜덤 추출한 정수에 해당하는 데이터를 추출 \n","    # -> 학습 데이터를 batch_size 만큼 랜덤하게 추출할 수 있음\n","    x_batch = x_train[batch_mask]\n","    t_batch = t_train[batch_mask]\n","    \n","    # 기울기 계산\n","    # 손실함수의 수치 gradient함수에 가중치와 편향을 넣은 결과를 딕셔너리로 리턴받음\n","    # grad = network.gradient(x_batch, t_batch) -> 역전파 이용\n","    grad = network.numerical_gradient(x_batch, t_batch)\n","\n","    # 매개변수 갱신\n","    for key in ('W1', 'b1', 'W2', 'b2'):\n","        network.params[key] -= learning_rate * grad[key]\n","    \n","    # 학습 경과 기록\n","    loss = network.loss(x_batch, t_batch)\n","    train_loss_list.append(loss)\n","    \n","    # 1 에폭 당 정확도 계산\n","    if i % iter_per_epoch == 0:\n","        train_acc = network.accuracy(x_train, t_train)\n","        test_acc = network.accuracy(x_test, t_test)\n","        train_acc_list.append(train_acc)\n","        test_acc_list.append(test_acc)\n","        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n","\n","# 그래프 그리기\n","markers = {'train': 'o', 'test': 's'}\n","x = np.arange(len(train_acc_list))\n","plt.plot(x, train_acc_list, label='train acc')\n","plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n","plt.xlabel(\"epochs\")\n","plt.ylabel(\"accuracy\")\n","plt.ylim(0, 1.0)\n","plt.legend(loc='lower right')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kAnXpkbZLOxN"},"outputs":[],"source":["'''\n","참고자료\n","https://frozenca.wordpress.com/2020/11/28/5-neural-networks/\n","https://m.blog.naver.com/vi_football/221878546500\n","http://norman3.github.io/prml/docs/chapter05/3.html\n","https://roadcom.tistory.com/87\n","http://incredible.ai/artificial-intelligence/2017/01/07/Deep-Learning/\n","https://yonsodev.tistory.com/11\n","https://github.com/WegraLee/deep-learning-from-scratch/blob/82457ee673646f58708068842e825530b7fd1640/common/functions.py#L1\n","https://www.youtube.com/watch?v=seoXj2abFDQ\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jLOnBttgPPvg"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Middle_4. Backpropagation_using_FDM.ipynb","provenance":[],"authorship_tag":"ABX9TyP/OhsCK8uLAZz23Da1EQ/t"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}